{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model with L2 regularization\n",
    "model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(10,)))\n",
    "model.add(Dense(units=32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer and binary cross-entropy loss function\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad regularization is a variant of gradient descent that adapts the learning rate of each weight in the network based on the historical gradients for that weight. It is an effective way to reduce the learning rate for weights that receive frequent updates and increase the learning rate for weights that receive infrequent updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model with L2 regularization\n",
    "model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(10,)))\n",
    "model.add(Dense(units=32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adagrad optimizer and binary cross-entropy loss function\n",
    "model.compile(optimizer=Adagrad(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop regularization is another variant of gradient descent that adapts the learning rate of each weight based on the moving average of the squared gradients for that weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model with L2 regularization\n",
    "model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(10,)))\n",
    "model.add(Dense(units=32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with RMSprop optimizer and binary cross-entropy loss function\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adadelta regularization is another variant of gradient descent that is similar to RMSprop. Adadelta computes a moving average of the gradients and a moving average of the updates, and it uses these to adjust the learning rate for each weight. It also includes a parameter called \"rho\" that controls the size of the moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model with L2 regularization\n",
    "model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(10,)))\n",
    "model.add(Dense(units=32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adadelta optimizer and binary cross-entropy loss function\n",
    "model.compile(optimizer=Adadelta(lr=1.0, rho=0.95), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesterov accelerated gradient (NAG) is a variant of gradient descent that uses momentum to accelerate convergence. The momentum term is calculated based on the previous gradient and the current gradient estimate. This can help to reduce oscillations and improve convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model with L2 regularization\n",
    "model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(10,)))\n",
    "model.add(Dense(units=32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with SGD optimizer and binary cross-entropy loss function\n",
    "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal Covariate Shift (ICS) is a phenomenon that occurs when the distribution of inputs to a layer changes during training. This can cause the weights of the layer to become suboptimal, leading to slower convergence and decreased accuracy.One approach to address ICS is called Batch Normalization. In Batch Normalization, the inputs to each layer are normalized to have zero mean and unit variance, independently of the other inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model with Batch Normalization\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(10,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary cross-entropy loss function and Adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
